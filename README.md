# PoC: Deploy GitLab Helm on HA Kubeadm Cluster using QEMU + KVM with Packer, Terraform, Vault, and Ansible

## Section 0. Introduction

This repository contains an Infrastructure as Code (IaC) Proof of Concept (PoC) utilizing QEMU-KVM to automate the deployment of a High Availability (HA) Kubernetes Cluster (Kubeadm) in an on-premise environment. This project was developed based on the requirements identified during an internship at Cathay General Hospital to automate the foundation for GitLab on-premise, targeting legacy systems with a repeatable and efficient IaC pipeline.

(The repository is hosted publicly here as a technical portfolio, in agreement with the organization.)

The machine specifications used for development are as follows, for reference:

-   **Chipset:** Intel® HM770
-   **CPU:** Intel® Core™ i7 processor 14700HX
-   **RAM:** Micron Crucial Pro 64GB Kit (32GBx2) DDR5-5600 UDIMM
-   **SSD:** WD PC SN560 SDDPNQE-1T00-1032

You can clone the project using the following command:

```shell
git clone https://github.com/csning1998/on-premise-gitlab-deployment.git
```

### A. Disclaimer

-   This project currently only works on Linux devices with CPU virtualization support, and has **not yet** been tested on other distros such as Fedora 41, Arch, CentOS, and WSL2.
-   Currently, these features have only been tested on my personal computer through several system reinstallations, so there may inevitably be some functionality issues. I've tried my best to prevent and address these.

### B. Prerequisites

Before you begin, ensure you have the following:

-   A Linux host (RHEL 10 or Ubuntu 24 recommended).
-   A CPU with virtualization support enabled (VT-x or AMD-V).
-   `sudo` access for Libvirt.
-   `podman` and `podman compose` installed (for containerized mode).
-   `whois` package installed (for the `mkpasswd` command).

### C. Note

This project requires CPU with virtualization support. For users whose CPUs don't support virtualization, you can refer to the `legacy-workstation-on-ubuntu` branch. This has been tested on Ubuntu 24 to achieve the same basic functionality. Use the following in shell to check if your device support virtualization:

```shell
lscpu | grep Virtualization
```

The output may show:

-   Virtualization: VT-x (Intel)
-   Virtualization: AMD-V (AMD)
-   If there is no output, virtualization might not be supported.

### D. The Entrypoint: `entry.sh`

The content in Section 1 and Section 2 serves as prerequisite setup before formal execution. Project lifecycle management and configuration are handled through the `entry.sh` script in the root directory. After executing `./entry.sh`, you will see the following content:

```text
➜  on-premise-gitlab-deployment git:(main) ✗ ./entry.sh
... (Some preflight check)

======= IaC-Driven Virtualization Management =======

Environment: NATIVE
Vault Server Status: Running (Unsealed)

1) [ONCE-ONLY] Set up CA Certs for TLS              9) Purge All Libvirt Resources
2) [ONCE-ONLY] Initialize Vault                    10) Purge All Packer and Terraform Resources
3) [ONCE-ONLY] Generate SSH Key                    11) Build Packer Base Image
4) [ONCE-ONLY] Setup KVM / QEMU for Native         12) Provision Terraform Layer
5) [ONCE-ONLY] Setup Core IaC Tools for Native     13) [DEV] Rebuild Layer via Ansible Command
6) [ONCE-ONLY] Verify IaC Environment for Native   14) Verify SSH
7) Unseal Vault                                    15) Quit
8) Switch Environment Strategy

>>> Please select an action:
```

Among options `11`, `12`, and `13`, there are submenus. These menus are dynamically created based on the directories under `packer/output` and `terraform/layers`. In the current complete setup, they are:

1. If you select `11) Build Packer Base Image`

    ```text
    >>> Please select an action: 11
    # Entering Packer build selection menu...
    #### Checking status of libvirt service...
    --> libvirt service is already running.

    1) Build ALL Packer Images  3) 02-base-kubeadm     5) 04-base-postgres    7) 06-base-minio       9) Back to Main Menu
    2) 01-base-docker           4) 03-base-microk8s    6) 05-base-redis       8) 07-base-vault

    >>> Please select an action:
    ```

2. If you select `12) Provision Terraform Layer`

    ```text
    >>> Please select an action: 12
    # Entering Terraform layer management menu...
    #### Checking status of libvirt service...
    --> libvirt service is already running.
    1) 10-vault-core          4) 20-gitlab-redis       7) 20-harbor-redis      10) 50-gitlab-kubeadm
    2) 20-gitlab-minio        5) 20-harbor-minio       8) 30-harbor-microk8s   11) 60-gitlab-platform
    3) 20-gitlab-postgres     6) 20-harbor-postgres    9) 40-provision-harbor  12) Back to Main Menu
    >>> Select a Terraform layer to REBUILD:
    ```

3. If you select `13) [DEV] Rebuild Layer via Ansible Command`

    ```text
    >>> Please select an action: 13
    # Executing [DEV] Rebuild via direct Ansible command...
    #### Checking status of libvirt service...
    --> libvirt service is already running.
    1) inventory-kubeadm-gitlab.yaml   4) inventory-minio-harbor.yaml     7) inventory-redis-gitlab.yaml
    2) inventory-microk8s-harbor.yaml  5) inventory-postgres-gitlab.yaml  8) inventory-vault-cluster.yaml
    3) inventory-minio-gitlab.yaml     6) inventory-postgres-harbor.yaml  9) Back to Main Menu
    >>> Select a Cluster Inventory to run its Playbook:
    ```

**A description of how to use this script follows below.**

## Section 1. Environmental Setup

### A. Required. KVM / QEMU

The user's CPU must support virtualization technology to enable QEMU-KVM functionality. You can choose whether to install it through option `4` via script (but this has only been tested on Ubuntu 24 and RHEL 10), or refer to relevant resources to set up the KVM and QEMU environment on your own.

### B. Option 1. Install IaC tools on Native

1. **Install HashiCorp Toolkit - Terraform and Packer**

    Next, you can install Terraform, Packer, and Ansible by running `entry.sh` in the project root directory and selecting option `5` for _"Setup Core IaC Tools for Native"_. Or alternatively, follow the offical installation guide:

    > _Reference: [Terraform Installation](https://developer.hashicorp.com/terraform/install)_  
    > _Reference: [Packer Installation](https://developer.hashicorp.com/packer/install)_ > _Reference: [Ansible Installation](https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html)_

    Expected output should reflect the latest versions. For instance (in zsh):

    ```text
    ...
    >>> Please select an action: 4
    >>> STEP: Verifying full native IaC environment...
    >>> Verifying Libvirt/KVM environment...
    #### QEMU/KVM: Installed
    #### Libvirt Client (virsh): Installed
    >>> Verifying Core IaC Tools (HashiCorp/Ansible)...
    #### HashiCorp Packer: Installed
    #### HashiCorp Terraform: Installed
    #### HashiCorp Vault: Installed
    #### Red Hat Ansible: Installed
    ```

### B. Option 2. Run IaC tools inside Container: Podman

1. Please ensure Podman is installed correctly. You can refer to the following URL and choose the installation method corresponding to your platform

2. After completing the Podman installation, please switch to the project root directory:

    1. Default memlock limit (`ulimit -l`) is usually too low for HashiCorp Vault. Rootless Podman inherits this low limit, causing Vault's mlock system call to fail. You can modify it by running the following command:

        ```shell
        sudo tee -a /etc/security/limits.conf <<EOT
        ${USER}    soft    memlock    unlimited
        ${USER}    hard    memlock    unlimited
        EOT
        ```

        And then reboot the system to take effect. This prevents sensitive data from being swapped to unencrypted swap space.

    2. If using for the first time, execute the following command

        ```shell
        podman compose up --build
        ```

    3. After creating the Container, you only need to run the container to perform operations:

        ```shell
        podman compose up -d
        ```

    4. Currently, the default setting is `DEBIAN_FRONTEND=noninteractive`. If you need to make any modifications and check inside the container, you can execute

        ```shell
        podman exec -it iac-controller-base bash
        ```

        Where `iac-controller-base` is the root Container name for the project.

    5. Default Container output after `podman compose up -d` and `podman ps -a` is akin to the following:

        ```text
        CONTAINER ID  IMAGE                                            COMMAND               CREATED         STATUS                   PORTS       NAMES
        61be68ae276e  docker.io/hashicorp/vault:1.20.2                 server -config=/v...  15 minutes ago  Up 15 minutes (healthy)  8200/tcp    iac-vault-server
        79b918f440f1  localhost/on-premise-iac-controller:qemu-latest  /bin/bash             15 minutes ago  Up 15 minutes                        iac-controller-base
        0a4eb3495697  localhost/on-premise-iac-controller:qemu-latest  /bin/bash             15 minutes ago  Up 15 minutes                        iac-controller-packer
        482f58b67295  localhost/on-premise-iac-controller:qemu-latest  /bin/bash             15 minutes ago  Up 15 minutes                        iac-controller-terraform
        aa8d17213095  localhost/on-premise-iac-controller:qemu-latest  /bin/bash             15 minutes ago  Up 15 minutes                        iac-controller-ansible
        ```

    6. **Attention: When switching between the Podman container and the native environment, ALL the Libvirt resources created by Terraform WILL BE DELETED to avoid conflicts.**

### C. Miscellaneous

-   **Suggested Plugins for VSCode:** Enhance productivity with syntax support:

    1. Ansible language support extension. [Marketplace Link of Ansible](https://marketplace.visualstudio.com/items?itemName=redhat.ansible)

        ```shell
        code --install-extension redhat.ansible
        ```

    2. HCL language support extension for Terraform. [Marketplace Link of HashiCorp HCL](https://marketplace.visualstudio.com/items?itemName=HashiCorp.HCL)

        ```shell
        code --install-extension HashiCorp.HCL
        ```

    3. Packer tool extension. [Marketplace Link of Packer Powertools](https://marketplace.visualstudio.com/items?itemName=szTheory.vscode-packer-powertools)

        ```shell
        code --install-extension szTheory.vscode-packer-powertools
        ```

## Section 2. Configuration

### Step A. Project Overview

To ensure the project runs smoothly, please follow the procedures below to complete the initialization setup.

0. **Environment variable file:** The script `entry.sh` will automatically create a `.env` environment variable that is used by for script files, which can be ignored.

1. **Generate SSH Key:** During the execution of Terraform and Ansible, SSH keys will be used for node access authentication and configuration management. You can generate these by running `./entry.sh` and entering `3` to access the _"Generate SSH Key"_ option. You can enter your desired key name or simply use the default value `id_ed25519_on-premise-gitlab-deployment`. The generated public and private key pair will be stored in the `~/.ssh` directory

2. **Switch Environment:** You can switch between "Container" or "Native" environment by using `./entry.sh` and entering `8`. Currently this project primarily uses Podman, and I _personally_ recommend decoupling the Podman and Docker runtime environments to prevent execution issues caused by SELinux-related permissions. For example, SELinux policies do not allow a `container_t` process that is common in Docker to connect to a `virt_var_run_t` Socket, which may cause Terraform Provider or `virsh` to receive "Permission Denied" errors when running in containers, even though everything appears normal from a filesystem permissions perspective.

### Step B. Set up Variables

#### **Step B.0. Examine the Permissions of Libvirt**

Libvirt's settings directly impact Terraform's execution permissions, thus some permission checks are required.

1. Ensure the user's account is a member of the `libvirt` group.

    ```shell
    sudo usermod -aG libvirt $(whoami)
    ```

    **Note:** After completing this step, you must fully log out and log back in, or restart your computer, for the new group membership to take effect in your shell session.

2. Modify the `libvirtd` configuration file to explicitly state that the `libvirt` group should manage the socket.

    ```shell
    # If you use vim
    sudo vim /etc/libvirt/libvirtd.conf
    # If you use nano
    sudo nano /etc/libvirt/libvirtd.conf
    ```

    Locate and uncomment (remove the `#` from the beginning of the line) the following two lines.

    ```toml
    unix_sock_group = "libvirt"
    # ...
    unix_sock_rw_perms = "0770"
    ```

3. Override the `systemd` socket unit settings, as `systemd`'s socket configuration takes precedence over `libvirtd.conf`.

    1. Executing the following command will open a `nano` editor.

        ```shell
        sudo systemctl edit libvirtd.socket
        ```

    2. In the opened editor, paste the following content. Make sure to paste it above the line that says `### Edits below this comment will be discarded` to prevent the configuration file from becoming invalid.

        ```toml
        [Socket]
        SocketGroup=libvirt
        SocketMode=0770
        ```

        Once done, use `Ctrl+O` to save and `Ctrl+X` to exit the editor.

4. Now, the services need to be restarted in the correct order for all the settings to take effect.

    1. Reload `systemd` configurations:

        ```shell
        sudo systemctl daemon-reload
        ```

    2. Stop all related services to ensure a clean state:

        ```shell
        sudo systemctl stop libvirtd.service libvirtd.socket libvirtd-ro.socket libvirtd-admin.socket
        ```

    3. Disable `libvirtd.service` to fully hand over management to Socket Activation:

        ```shell
        sudo systemctl disable libvirtd.service
        ```

    4. Restart `libvirtd.socket`

        ```shell
        sudo systemctl restart libvirtd.socket
        ```

5. Verification

    1. Check the socket permissions: The output should show the group as `libvirt` and the permissions as `srwxrwx---`.

        ```shell
        ls -la /var/run/libvirt/libvirt-sock
        ```

    2. Execute `virsh` commands as a **non-root** user.

        ```shell
        virsh list --all
        ```

        If the command executes successfully and lists the virtual machines (even if the list is empty), it means you have all the necessary permissions.

#### **Step B.1. Create Confidential Variable File for HashiCorp Vault**

> _**All secrets will be integrated into HashiCorp Vault. This project defaults to using Vault with HTTPS configuration. Please follow the steps below to ensure correct setup.**_

1. First, run the `entry.sh` script and select option `1` to generate the files required for the TLS handshake. When creating the self-signed CA certificate, you can leave the fields blank for now. If you need to regenerate the TLS files, you can simply run option `1` again.

2. Switch to the project's root directory and run the following command to start the Vault server.

    - If you prefer to run it on the host:

        ```shell
        vault server -config=vault/vault.hcl
        ```

    - If you prefer to run it in a container (side-car mode):

        ```shell
        podman compose up -d iac-vault-server
        ```

    After starting the server, Vault will create `vault.db` and Raft-related files in the `vault/data/` directory. If you need to reinitialize Vault, you must manually clear all files within the `vault/data/` directory.

    **Note:** Please open a new terminal window or tab for subsequent operations.

3. Once the previous steps are complete, you can run `entry.sh` and select option `2` to initialize Vault. This process will also automatically perform the required unseal action for Vault.

4. Next, you only need to manually modify the following variables used in the project.

    - **For Common Variables in this Project**

        ```shell
        vault kv put \
            -address="https://127.0.0.1:8200" \
            -ca-cert="${PWD}/vault/tls/ca.pem" \
            secret/on-premise-gitlab-deployment/variables \
            ssh_username="some-user-name-for-ssh" \
            ssh_password="some-user-password-for-ssh" \
            ssh_password_hash=$(echo -n "$ssh_password" | mkpasswd -m sha-512 -P 0) \
            vm_username="some-user-name-for-vm" \
            vm_password="some-user-password-for-vm" \
            ssh_public_key_path="~/.ssh/some-ssh-key-name.pub" \
            ssh_private_key_path="~/.ssh/some-ssh-key-name"
        ```

    - **For Databases**

        ```shell
        vault kv put \
            -address="https://127.0.0.1:8200" \
            -ca-cert="${PWD}/vault/tls/ca.pem" \
            secret/on-premise-gitlab-deployment/databases \
            pg_superuser_password="a-more-secure-pwd-for-superuser" \
            pg_replication_password="a-more-secure-pwd-for-replication" \
            pg_vrrp_secret="a-more-secure-pwd-for-vrrpsecret" \
            redis_requirepass="a-more-secure-pwd-for-requirepass" \
            redis_masterauth="a-more-secure-pwd-for-masterauth" \
            redis_vrrp_secret="a-more-secure-pwd-for-vrrpsecret" \
            minio_root_password="a-more-secure-pwd-for-rootpassword" \
            minio_vrrp_secret="a-more-secure-pwd-for-vrrpsecret"
        ```

    - **For Infrastructure Variables in this Project**

        ```shell
        vault kv put \
            -address="https://127.0.0.1:8200" \
            -ca-cert="${PWD}/vault/tls/ca.pem" \
            secret/on-premise-gitlab-deployment/infrastructure \
            vault_keepalived_auth_pass="a-more-secure-pwd-for-keepalivedauthpass" \
            vault_haproxy_stats_pass="a-more-secure-pwd-for-haproxystatspass"
        ```

    - **For Harbor Variables in this Project**

        ```shell
        vault kv put \
            -address="https://127.0.0.1:8200" \
            -ca-cert="${PWD}/vault/tls/ca.pem" \
            secret/on-premise-gitlab-deployment/harbor \
            harbor_admin_password="a-more-secure-pwd-for-harbor_admin_password" \
            harbor_pg_db_password="a-more-secure-pwd-for-harbor_pg_db_password"
        ```

    - **Note 1:**

        In which `ssh_username` and `ssh_password` are the account and password used to log into the virtual machine; while `ssh_password_hash` is the hashed password required for virtual machine automatic installation (Cloud-init). This password needs to be generated using the password string from `ssh_password`. For instance, if the password is `HelloWorld@k8s`, then the corresponding password hash should be generated using the following command:

        ```shell
        mkpasswd -m sha-512 HelloWorld@k8s
        ```

        If it shows `mkpasswd` command not found, possibly due to lack of `whois` package.

        In which `ssh_public_key_path` needs to be changed to the **public key** name generated earlier, the public key will be in `*.pub` format.

    - **Note 2:**

        The current SSH identity variables are primarily used for Packer in a single-use scenario, while the VM identity variables are used by Terraform when cloning VMs. In principle, they can be set to the same value. However, if you need to set different names for different VMs, you can directly modify the objects and relevant code in HCL. Typically, you would modify the `ansible_runner.vm_credentials` variable and related variable passing, and then use a `for_each` loop for iteration. This increases complexity, so if there are no other requirements, it is recommanded to keep the SSH and VM identity variables the same.

5. When starting the project, you only need to launch the Vault server in a single terminal (maybe in your IDE) window using `vault server -config=vault/vault.hcl`. Afterward, you can use option `7` in `entry.sh` to unseal the Vault database; Alternatively, you may use container as described in B.1-2, which is suggested due to simplicity.

#### **Step B.2. Create Variable File for Terraform:**

1. You can RENAME the `terraform/layers/*/terraform.tfvars.example` file to `terraform/layers/*/terraform.tfvars` file using the following command:

    ```shell
    for f in terraform/layers/*/terraform.tfvars.example; do cp -n "$f" "${f%.example}"; done
    ```

    Then, you can modify the `terraform.tfvars` file based on your requirements. HA and non-HA are both supported.

2. For users setting up an HA Cluster (GitLab as example), the number of elements in `gitlab_kubeadm_compute.masters` and `gitlab_kubeadm_compute.workers` determines the number of nodes generated. Ensure the quantity of nodes in `gitlab_kubeadm_compute.masters` is an odd number to prevent the etcd Split-Brain risk in Kubernetes. Meanwhile, `gitlab_kubeadm_compute.workers` can be configured based on the number of IPs. The IPs provided by the user must correspond to the host-only network segment.

    - The latest version is available at <https://cdimage.ubuntu.com/ubuntu/releases/24.04/release/> ,
    - The test version of this project is also available at <https://old-releases.ubuntu.com/releases/noble/> .
    - After selecting your version, please verify the checksum.
        - For latest Noble version: <https://releases.ubuntu.com/noble/SHA256SUMS>
        - For "Noble-old-release" version: <https://old-releases.ubuntu.com/releases/noble/SHA256SUMS>

    Deploying other Linux distro would be supported if I have time. I'm still a full-time university student.

3. **[Example] To deploy a complete GitLab HA Cluster from scratch**:

    - **First Step**: Enter the main menu `11) Build Packer Base Image`, then select `02-base-kubeadm` to build the base image required by Kubeadm.

    - **Second Step**: After the previous step is complete, return to the main menu and enter `12) Provision Terraform Layer`, then select `50-gitlab-kubeadm` to deploy the entire GitLab cluster (Now only kubeadm is implemented).

        Based on testing, this complete process (from building the Packer image to completing the Terraform deployment) takes approximately 7 minutes.

4. **Isolated Testing and Development**:

    The `11`, `12`, and `13` menus can be used for separate testing

    - To test Packer image building independently, use `11) Build Packer Base Image`.

    - To test or rebuild a specific Terraform module layer independently (such as Harbor or Postgres), use `12) Provision Terraform Layer`.

    - To repeatedly test Ansible Playbooks on existing machines without recreating virtual machines, use `13) [DEV] Rebuild Layer via Ansible Command`.

5. **Resource Cleanup**:

    - **`9) Purge All Libvirt Resources`**:

        This option executes `libvirt_resource_purger "all"`, which **only deletes** all virtual machines, virtual networks, and storage pools created by this project, but **will preserve** Packer's output images and Terraform's local state files. This is suitable for scenarios where you only want to clean up virtualization resources without clearing the project state.

    - **`10) Purge All Packer and Terraform Resources`**:

        This option deletes **all** Packer output images and **all** Terraform Layer local states, causing Packer and Terraform states in this project to be restored to an almost brand new state.

## Section 3. System Architecture

This project employs three tools - Packer, Terraform, and Ansible - using an Infrastructure as Code (IaC) approach to achieve a fully automated workflow from virtual machine image creation to Kubernetes cluster deployment. The overall architecture follows the principle of Immutable Infrastructure, ensuring that each deployment environment is consistent and predictable.

### A. Deployment Workflow

The entire automated deployment process is triggered by option `12` _"Rebuild Kubeadm Cluster (Packer + TF)
"_ in the `./entry.sh` script, with detailed steps shown in the diagram below:

```mermaid
sequenceDiagram
    actor User
    participant Entrypoint as entry.sh
    participant Packer
    participant Terraform
    participant Ansible
    participant Libvirt as Libvirt/QEMU

    User->>+Entrypoint: Execute 'Rebuild All'

    Entrypoint->>+Packer: 1. Execute 'packer_build_executor'
    Packer->>+Libvirt: 1a. Build VM from ISO
    note right of Packer: Provisioner 'ansible' is triggered
    Packer->>+Ansible: 1b. Execute Playbook<br>(00-provision-base-image.yaml)
    Ansible-->>-Packer: (Bake k8s components into image)
    Libvirt-->>-Packer: 1c. Output Golden Image (.qcow2)
    Packer-->>-Entrypoint: Image creation complete

    Entrypoint->>+Terraform: 2. Execute 'apply_terraform_10-cluster-provision'
    note right of Terraform: Reads .tf definitions
    Terraform->>+Libvirt: 2a. Create Network, Pool, Volumes (from .qcow2), Cloud-init ISOs
    Terraform->>+Libvirt: 2b. Create and Start VMs (Domains)
    note right of Terraform: Provisioner 'local-exec' is triggered
    Terraform->>+Ansible: 2c. Execute Playbook<br>(10-provision-kubeadm.yaml)
    Ansible->>Libvirt: (via SSH) 2d. Configure HA (Keepalived/HAProxy)
    Ansible->>Libvirt: (via SSH) 2e. Init/Join Kubernetes Cluster
    Ansible-->>-Terraform: Playbook execution complete
    Terraform-->>-Entrypoint: 'apply' complete
    Entrypoint-->>-User: Display 'Rebuild All workflow completed'
```

### B. Toolchain Roles and Responsibilities (Outdated)

**This section is outdated. The architecture is now nearly converged, this section will be updated once the PoC of GitLab Infrastructure is ready.**

> The setup process is based on the commands provided by Bibin Wilson (2025), which I implemented using an Ansible Playbook. Thanks to the author, Bibin Wilson, for the contribution on his article
>
> Work Cited: Bibin Wilson, B. (2025). _How To Setup Kubernetes Cluster Using Kubeadm._ devopscube. <https://devopscube.com/setup-kubernetes-cluster-kubeadm/#vagrantfile-kubeadm-scripts-manifests>

1. **Packer + Ansible: Provisioning base Kubernetes Golden Image**

    Packer plays the role of an "image factory" in this project, with its core task being to automate the creation of a standardized virtual machine template (Golden Image) pre-configured with all Kubernetes dependencies. The project uses `packer/source.pkr.hcl` as its definition file and it's driven by `packer/02-base-kubeadm.pkrvars.hcl`, with a workflow that includes: automatically downloading the `Ubuntu Server 24.04 ISO` file and completing unattended installation using cloud-init; starting SSH connection and invoking the Ansible Provisioner after installation; executing `ansible/playbooks/00-provision-base-image.yaml` to install necessary components such as `kubelet`, `kubeadm`, `kubectl`, and `CRI-O` (also configure it to use `cgroup` driver); finally shutting down the virtual machine and producing a `*.qcow2` format template for Terraform to use. The goal of this phase is to "bake" all infrequently changing software and configurations into the image to reduce the time required for subsequent deployments.

2. **Terraform: The Infrastructure Orchestrator**

    Terraform is responsible for managing the infrastructure lifecycle and serves as the core orchestration component of the entire architecture. Terraform reads the image template produced by Packer and deploys the actual virtual machine cluster in Libvirt/QEMU. The definition files are the `.tf` files in the `terraform/` directory, with the **workflow as follows:**

    - **Node Deployment (Layer 10)**:

        - Based on `kubeadm_cluster_config` defined in `terraform/terraform.tfvars`, Terraform calculates the number of nodes that need to be created.
        - Next, Terraform's libvirt provider will quickly clone virtual machines based on the `.qcow2` file. Under the hardware resources listed in Section 0, cloning 6 virtual machines can be completed in approximately 15 seconds.

    - **Cluster Configuration (Layer 50)**:
        - Once all nodes are ready, Terraform dynamically generates `ansible/inventory.yaml` list file.
        - Then, Terraform invokes Ansible to execute the `ansible/playbooks/10-provision-kubeadm.yaml` Playbook to complete the initialization of the Kubernetes cluster.

3. **Ansible: The Configuration Manager**

    This is the twice call for Ansible, serving as the configuration manager at different stages. The project's Playbooks are stored in the `ansible/playbooks/ directory`. In terms of role assignment, Ansible is primarily responsible for cluster initialization (invoked by Terraform), executing the following tasks through the `10-provision-kubeadm.yaml` Playbook:

    1. Setup HA Load Balancer on all master nodes if it's not a cluster with single master node.
    2. Initialize the primary master node
    3. Generate and fetch join commands from primary master
    4. Executing `kubeadm join` on
        1. Other master node if it's HA Cluster
        2. Worker nodes to join them to the cluster.

4. **HashiCorp Vault (with TLS)**

    HashiCorp Vault is integrated into this project to serve as a centralized and secure backend for managing all sensitive data, such as SSH credentials and user passwords. This approach removes the need to store plaintext secrets in version-controlled files like `*.tfvars` or `*.pkrvars.hcl`, aligning with Infrastructure as Code best practices.

    Both the `native` (host-based) and `container` (Podman-based) IaC execution strategies can connect to the same containerized Vault instance through the host network (`network_mode: "host"`). The Vault server is configured to start automatically on system boot if you use container, ensuring the secrets backend is always available for the IaC workflow without manual intervention.

    - **Packer:** During the image-building phase, Packer authenticates with the Vault server and uses the `vault()` function within its HCL files to dynamically fetch the necessary secrets (e.g., `ssh_username`, `ssh_password_hash`) required for the unattended OS installation.

    - **Terraform:** Similarly, Terraform utilizes the Vault Provider to read infrastructure-related secrets, such as `vm_username` and `ssh_private_key_path`, directly from Vault at runtime. This allows Terraform to provision VMs and configure SSH access without exposing any sensitive credentials in its configuration files.

### C. Infrastructures

_**(To be continued...)**_
